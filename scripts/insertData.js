/**
 * =============================================================================
 *  insertFullExport.js ‚Äî IMPORTADOR PROFESIONAL + SINCRONIZACI√ìN DE COUNTERS
 * =============================================================================
 *
 *  OBJETIVO
 *  --------
 *  Importar de forma segura y eficiente un archivo JSON que contiene:
 *
 *      {
 *          "companies": [...],
 *          "jobs": [...]
 *      }
 *
 *  y dejar el sistema en un estado CONSISTENTE para operaci√≥n normal del backend.
 *
 *  Este script:
 *   ‚úî Elimina SOLO las colecciones `companies` y `jobs`
 *   ‚úî Inserta datos en LOTES (chunked inserts)
 *   ‚úî Muestra barra de progreso real
 *   ‚úî NO normaliza ni limpia datos (se asume JSON correcto)
 *   ‚úî SINCRONIZA los contadores incrementales (`company_id`, `job_id`)
 *
 *  Al finalizar:
 *   - Los IDs incrementales quedan alineados con el MAX real en la BD
 *   - El backend puede seguir creando empresas y vacantes sin colisiones
 *
 * =============================================================================
 */

import fs from "fs";
import path from "path";
import mongoose from "mongoose";

// Modelos principales
import Company from "../models/Company.js";
import Job from "../models/Job.js";

// Modelo de contadores incrementales
import Counter from "../models/Counter.js";

// Infraestructura com√∫n
import { connectDB } from "../connection/db.js";
import { logger } from "../utils/logger.js";
import { ProgressBar } from "../utils/progressBar.js";
import { createPromptFromArgs } from "../utils/prompt.js";

// =============================================================================
// CONFIGURACI√ìN GENERAL
// =============================================================================

const prompt = createPromptFromArgs(process.argv);
const __dirname = path.resolve();

// Archivo de entrada (export previamente generado)
const FILE_PATH = path.join(__dirname, "data", "jobs.json");

// Tama√±o de lote para inserciones masivas
// Ajustable seg√∫n RAM / tama√±o del dataset
const CHUNK_SIZE = 2000;

// =============================================================================
// LIMPIEZA CONTROLADA DE COLECCIONES
// =============================================================================

/**
 * Elimina √öNICAMENTE las colecciones:
 *   - companies
 *   - jobs
 *
 * No toca:
 *   - users
 *   - applications
 *   - counters
 *   - ninguna otra colecci√≥n
 */
async function limpiarColecciones() {
    logger.section("Eliminando colecciones objetivo");

    const confirmado = await prompt.confirm(
        "Esto ELIMINAR√Å 'companies' y 'jobs'. ¬øContinuar? (y/N): "
    );

    if (!confirmado) {
        logger.warn("Operaci√≥n cancelada por el usuario.");
        process.exit(0);
    }

    const coleccionesObjetivo = ["companies", "jobs"];

    for (const nombre of coleccionesObjetivo) {
        const coleccion = mongoose.connection.collections[nombre];

        if (!coleccion) {
            logger.warn(`‚ùó La colecci√≥n '${nombre}' no existe. Se omite.`);
            continue;
        }

        logger.info(`üßπ Eliminando colecci√≥n '${nombre}'...`);

        await coleccion.drop().catch(err => {
            if (err.code === 26) {
                logger.warn(`(omitido) '${nombre}' no exist√≠a.`);
            } else {
                throw err;
            }
        });
    }

    logger.success("‚úî Colecciones eliminadas correctamente.");
}

// =============================================================================
// CARGA Y VALIDACI√ìN DEL EXPORT
// =============================================================================

/**
 * Carga el archivo JSON y valida su estructura m√≠nima.
 *
 * NO valida contenido de campos (eso se asume correcto).
 */
function cargarExportacion() {
    if (!fs.existsSync(FILE_PATH)) {
        throw new Error(`‚ùå No se encontr√≥ el archivo: ${FILE_PATH}`);
    }

    const contenido = fs.readFileSync(FILE_PATH, "utf8");
    const json = JSON.parse(contenido);

    if (!json.companies || !json.jobs) {
        throw new Error(
            "‚ùå Formato inv√°lido. Se esperaba: { companies: [...], jobs: [...] }"
        );
    }

    logger.success(
        `‚úî Archivo cargado ‚Üí Empresas: ${json.companies.length} | Trabajos: ${json.jobs.length}`
    );

    return json;
}

// =============================================================================
// INSERCI√ìN EN LOTES (CHUNKED INSERTS)
// =============================================================================

/**
 * Inserta documentos en bloques para:
 *  - No saturar memoria
 *  - Mantener la app responsiva
 *  - Permitir progreso real
 */
async function insertChunked(Model, data, progress, insertados) {
    for (let i = 0; i < data.length; i += CHUNK_SIZE) {
        const slice = data.slice(i, i + CHUNK_SIZE);

        await Model.insertMany(slice);

        insertados.count += slice.length;
        progress.update(insertados.count);
    }
}

// =============================================================================
// SINCRONIZACI√ìN DE CONTADORES INCREMENTALES
// =============================================================================

/**
 * Alinea los contadores (`Counter`) con el valor m√°ximo real insertado.
 *
 * Esto es CR√çTICO para:
 *  - evitar colisiones de IDs
 *  - permitir POST normales despu√©s del import
 */
async function syncCounters() {
    logger.section("Sincronizando counters incrementales");

    // M√°ximo company_id real
    const maxCompany = await Company.findOne({})
        .sort({ company_id: -1 })
        .select({ company_id: 1 })
        .lean();

    // M√°ximo job_id real
    const maxJob = await Job.findOne({})
        .sort({ job_id: -1 })
        .select({ job_id: 1 })
        .lean();

    const companySeq = maxCompany?.company_id ?? 0;
    const jobSeq = maxJob?.job_id ?? 0;

    // Actualiza o crea los counters
    await Counter.updateOne(
        { _id: "company_id" },
        { $set: { seq: companySeq } },
        { upsert: true }
    );

    await Counter.updateOne(
        { _id: "job_id" },
        { $set: { seq: jobSeq } },
        { upsert: true }
    );

    logger.success(`‚úî Counter company_id = ${companySeq}`);
    logger.success(`‚úî Counter job_id     = ${jobSeq}`);
}

// =============================================================================
// FLUJO PRINCIPAL
// =============================================================================

async function insertarDatos() {
    await connectDB();
    await limpiarColecciones();

    const data = cargarExportacion();
    const totalItems = data.companies.length + data.jobs.length;

    logger.section("Insertando datos");
    const progress = new ProgressBar(totalItems);
    const insertados = { count: 0 };

    // Inserta empresas
    if (data.companies.length > 0) {
        await insertChunked(Company, data.companies, progress, insertados);
    }

    // Inserta trabajos
    if (data.jobs.length > 0) {
        await insertChunked(Job, data.jobs, progress, insertados);
    }

    progress.finish();

    // üî• PASO CLAVE: alinear incrementadores
    await syncCounters();

    logger.success("üéâ Importaci√≥n completa y sistema consistente.");
    process.exit(0);
}

// =============================================================================
// EJECUCI√ìN
// =============================================================================

insertarDatos().catch(err => {
    logger.error(`‚ùå Error durante la importaci√≥n: ${err.message}`);
    process.exit(1);
});
